{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_CPP_MIN_LOG_LEVEL=3\n",
      "Python Version 3.11.7 (main, Dec  8 2023, 18:56:58) [GCC 11.4.0]\n",
      "Keras Version 3.0.4 with jax backend \tJax Version 0.4.23\n",
      "Jax backend device gpu\n"
     ]
    }
   ],
   "source": [
    "%env TF_CPP_MIN_LOG_LEVEL=3\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "import random as r\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import jax\n",
    "from jax import Array, numpy as jnp, random as jrand\n",
    "import keras as nn\n",
    "nn.utils.set_random_seed(812)\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Python Version\", sys.version); del sys\n",
    "print(f\"Keras Version {nn.__version__} with {nn.backend.backend()} backend \\tJax Version {jax.__version__}\")\n",
    "print(\"Jax backend device\", jax.default_backend())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding:\n",
    "    \"\"\"```\n",
    "    Sinusoidal Fixed Positional Embeddings\n",
    "    Args:\n",
    "        maxlen:int\n",
    "        dim:int\n",
    "    sinusoidal_embeddings: \n",
    "        pos_emb: (1, maxlen, dim)\n",
    "    get_freqs:\n",
    "        get_freqs: sin_freqs(1, maxlen, 1, dim), cos_freqs(1, maxlen, 1, dim)\n",
    "    ```\"\"\"\n",
    "    def __init__(self, maxlen:int, dim:int):\n",
    "        p, i = jnp.meshgrid(jnp.arange(float(maxlen)), jnp.arange(dim/2)*2)\n",
    "        theta = (p/1e4**(i/dim)).T\n",
    "\n",
    "        self.pos_emb = jnp.stack([jnp.sin(theta), jnp.cos(theta)], axis=-1)\n",
    "        self.pos_emb = self.pos_emb.reshape((maxlen, dim))[None] # (1, maxlen, dim)\n",
    "\n",
    "    def sinusoidal_embeddings(self):\n",
    "        return self.pos_emb # (1, maxlen, dim)\n",
    "    \n",
    "    def get_freqs(self):\n",
    "        sin_freqs = jnp.repeat(self.pos_emb[..., None, ::2], repeats=2, axis=-1)\n",
    "        cos_freqs = jnp.repeat(self.pos_emb[..., None, 1::2], repeats=2, axis=-1)\n",
    "        return sin_freqs, cos_freqs # (1, maxlen, 1, dim), (1, maxlen, 1, dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Layer):\n",
    "    \"\"\"```\n",
    "    Multi-head Attention\n",
    "    Args:\n",
    "        causal:bool\n",
    "        n_heads:int\n",
    "        d_model:int\n",
    "        maxlen:int\n",
    "        dropout_rate:float\n",
    "    Input:\n",
    "        inp2q: shape(B, N, d_model)\n",
    "        inp2k: shape(B, T, d_model)\n",
    "        inp2v: shape(B, T, d_model)\n",
    "    Output:\n",
    "        linear_att_out: shape(B, N, d_model)\n",
    "    ```\"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            causal:bool,\n",
    "            num_heads:int,\n",
    "            d_model:int,\n",
    "            maxlen:int,\n",
    "            dropout_rate:float,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        assert d_model % num_heads == 0\n",
    "        self.causal = causal\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.dim = self.d_model//num_heads\n",
    "\n",
    "        self.wq = nn.layers.Dense(self.d_model, use_bias=False)\n",
    "        self.wk = nn.layers.Dense(self.d_model, use_bias=False)\n",
    "        self.wv = nn.layers.Dense(self.d_model, use_bias=False)\n",
    "        self.attwei_dropout = nn.layers.Dropout(dropout_rate)\n",
    "\n",
    "        self.w = nn.layers.Dense(d_model)\n",
    "        if causal:\n",
    "            self.causal_mask = jnp.triu(jnp.full(shape=(1, 1, maxlen, maxlen), fill_value=-jnp.inf), k=1)\n",
    "\n",
    "    def call(\n",
    "            self,\n",
    "            inp2q:Array, # (B, T, d_model)\n",
    "            inp2k:Array, # (B, T, d_model)\n",
    "            inp2v:Array, # (B, T, d_model)\n",
    "    ):\n",
    "        B, T = inp2q.shape[:-1]\n",
    "        assert inp2k.shape == inp2v.shape == inp2q.shape\n",
    "\n",
    "        # compute q, k, v\n",
    "        q = self.wq(inp2q) # (B, T, d_model)\n",
    "        k = self.wk(inp2k) # (B, T, d_model)\n",
    "        v = self.wv(inp2v) # (B, T, d_model)\n",
    "\n",
    "        # seperate heads\n",
    "        q = q.reshape((B, self.num_heads, T, self.dim)) # (B, h, T, dim)\n",
    "        k = k.reshape((B, self.num_heads, T, self.dim)) # (B, h, T, dim)\n",
    "        v = v.reshape((B, self.num_heads, T, self.dim)) # (B, h, T, dim)\n",
    "\n",
    "        # compute attention weights\n",
    "        att_wei = (q @ k.transpose((0, 1, 3, 2)))/self.dim**0.5 # (B, h, T, T) <= (B, h, T, dim) @ (B, h, T, dim).transpose(2, 3)\n",
    "        if self.causal:\n",
    "            att_wei = att_wei + self.causal_mask[:, :, :T, :T] # (B, h, T, T)\n",
    "\n",
    "        att_wei = jax.nn.softmax(att_wei, axis=-1) # (B, h, T, T)\n",
    "        att_wei = self.attwei_dropout(att_wei)\n",
    "\n",
    "        # apply attention weights to v\n",
    "        att_out = att_wei @ v # (B, h, T, T) @ (B, h, T, dv) => (B, h, T, dv)\n",
    "        # combine heads\n",
    "        att_out = att_out.reshape((B, T, self.d_model)) # (B, T, h*dv) ==> (B, T, d_model)\n",
    "\n",
    "        # linear of att_out\n",
    "        linear_att_out = self.w(att_out) # (B, T, d_model)\n",
    "        return linear_att_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model:nn.Model, input_shape:tuple, val_intervals:tuple):\n",
    "    blabla = model(\n",
    "        jrand.randint(jrand.PRNGKey(32344), shape=input_shape, minval=val_intervals[0], maxval=val_intervals[-1])\n",
    "    ); del blabla\n",
    "    return model\n",
    "\n",
    "\n",
    "class TieWeights(nn.Layer):\n",
    "    def __init__(self, dense:nn.Layer, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dense = dense # (1, vocab_size, d_model)\n",
    "        self.activation = nn.activations.get(activation)\n",
    "        \n",
    "    def call(self, x): # (B, T, d_model)\n",
    "        W = jnp.array(self.dense.weights)[0].T # (d_model, vocab_size)\n",
    "        return self.activation(x @ W)\n",
    "\n",
    "\n",
    "class Block(nn.Model):\n",
    "    \"\"\"```\n",
    "    Block\n",
    "    Args:\n",
    "        causal:bool\n",
    "        num_heads:int\n",
    "        d_model:int\n",
    "        maxlen:int\n",
    "        dropout_rate:float\n",
    "        use_bias:bool\n",
    "    Inputs: \n",
    "        inputs: shape(B, T, d_model)\n",
    "        pad_mask: shape(B, T) of boolean dtype\n",
    "    Outputs:\n",
    "        outputs: shape(B, T, d_model)\n",
    "    ```\"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            causal: bool, \n",
    "            num_heads: int, \n",
    "            d_model: int,\n",
    "            maxlen:int,\n",
    "            dropout_rate: float,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        dff_in = 4*d_model\n",
    "        self.rmsnorm1 = nn.layers.LayerNormalization(rms_scaling=True, epsilon=1e-5)\n",
    "        self.rmsnorm2 = nn.layers.LayerNormalization(rms_scaling=True, epsilon=1e-5)\n",
    "\n",
    "        self.mha = Attention(causal, num_heads, d_model, maxlen, dropout_rate)\n",
    "\n",
    "        self.ffn = [\n",
    "            nn.layers.Dense(int(dff_in), use_bias=True),\n",
    "            nn.layers.Activation(nn.activations.gelu),\n",
    "            nn.layers.Dense(d_model, use_bias=True),\n",
    "            nn.layers.Dropout(dropout_rate)\n",
    "        ]\n",
    "        \n",
    "        self.dropout = nn.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs:Array):\n",
    "        z = self.rmsnorm1(inputs)\n",
    "        x = inputs+self.mha(z, z, z)\n",
    "\n",
    "        z = self.rmsnorm2(x)\n",
    "        for layer in self.ffn:\n",
    "            z = layer(z)\n",
    "        outputs = x + z\n",
    "        return outputs # (B, T, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Model):\n",
    "    \"\"\"```\n",
    "    GPT!!\n",
    "    Args:\n",
    "        args:\n",
    "            d_model: int\n",
    "            num_heads: int\n",
    "            num_layers: int\n",
    "            max_context_length: int\n",
    "            vocab_size: int\n",
    "            output_units: int => if None is equal to vocab_size\n",
    "            dropout_rate: float\n",
    "        causal:bool\n",
    "        output_activation:str\n",
    "    Inputs:\n",
    "        inputs: shape(B, T)\n",
    "    Outputs: \n",
    "        outputs: shape(B, T, d_model)    \n",
    "    ```\"\"\"\n",
    "    def __init__(self, args, causal:bool, output_activation:str=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        if args.output_units is None:\n",
    "            args.output_units = args.vocab_size\n",
    "        # pos embedding\n",
    "        self.pos_embed = nn.layers.Embedding(args.max_context_length, args.d_model)\n",
    "        # embedding tokens and dropout\n",
    "        self.emb_dropout = nn.layers.Dropout(args.dropout_rate)\n",
    "        self.token_embed = nn.layers.Embedding(args.vocab_size, args.d_model)\n",
    "\n",
    "        # block layers\n",
    "        self.block_layers = [\n",
    "                Block(\n",
    "                    causal=causal, \n",
    "                    num_heads=args.num_heads,\n",
    "                    d_model=args.d_model,\n",
    "                    maxlen=args.max_context_length,\n",
    "                    dropout_rate=args.dropout_rate,\n",
    "                ) for _ in range(args.num_layers)\n",
    "            ]\n",
    "\n",
    "        # final layer: outputs logits with dropout\n",
    "        self.norm = nn.layers.LayerNormalization(rms_scaling=True)\n",
    "        self.linear = nn.layers.Dense(args.output_units, activation=output_activation, use_bias=True) #TieWeights(self.token_embed, activation=output_activation)\n",
    "        self.logits_dropout = nn.layers.Dropout(rate=args.dropout_rate)\n",
    "\n",
    "    def call(\n",
    "            self, \n",
    "            inputs:Array\n",
    "    ):\n",
    "        # embed tokens\n",
    "        x = self.emb_dropout(self.token_embed(inputs)+self.pos_embed(jnp.arange(inputs.shape[-1]))) # (B, T) ==> (B, T, d_model)\n",
    "        \n",
    "        # block layers\n",
    "        for layer in self.block_layers:\n",
    "            x = layer(x) # (B, T, d_model) ==> (B, T, d_model)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # linear layer: outputs logits\n",
    "        logits = self.linear(x) # (B, T, d_model) ==> (B, T, vocab_size)\n",
    "        return self.logits_dropout(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = \"https://homl.info/shakespeare\"\n",
    "filepath = nn.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "shakespeare_text = open(filepath, \"r\", encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(shakespeare_text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(shakespeare_text)))\n",
    "VOCAB_SIZE = len(chars)\n",
    "print(''.join(chars))\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(100_000, seed=seed)\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56, 43, 1, 61, 43, 1, 54, 56, 53, 41, 43, 43, 42, 1, 39, 52, 63, 1, 44, 59, 56, 58, 46, 43, 56, 6, 1, 46, 43, 39, 56, 1, 51, 43, 1, 57, 54, 43, 39, 49, 8, 0, 0, 13, 50, 50, 10, 0, 31, 54, 43, 39, 49, 6, 1, 57, 54, 43, 39, 49, 8, 0, 0, 18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 37, 53, 59]\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encode(shakespeare_text[:100]))\n",
    "print(decode(encode(shakespeare_text[:100])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTArgs(d_model=128, num_layers=4, num_heads=8, max_context_length=64, vocab_size=65, output_units=None, dropout_rate=0.4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class GPTArgs:\n",
    "    \"\"\"GPT Configuration\"\"\"\n",
    "    d_model:int = 128                       \n",
    "    num_layers:int = 4\n",
    "    num_heads:int = 8\n",
    "    max_context_length:int = 64\n",
    "    vocab_size:int = VOCAB_SIZE\n",
    "    output_units:int = None\n",
    "    assert d_model % 2 == 0\n",
    "    assert d_model % num_heads == 0\n",
    "    dropout_rate:float = 0.4\n",
    "\n",
    "GPTArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gpt\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gpt\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">    Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ block (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)                   │ ?                         │    <span style=\"color: #00af00; text-decoration-color: #00af00\">197,632</span> │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ block_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)                 │ ?                         │    <span style=\"color: #00af00; text-decoration-color: #00af00\">197,632</span> │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ block_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)                 │ ?                         │    <span style=\"color: #00af00; text-decoration-color: #00af00\">197,632</span> │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ block_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Block</span>)                 │ ?                         │    <span style=\"color: #00af00; text-decoration-color: #00af00\">197,632</span> │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ layer_normalization_8           │ ?                         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                           │            │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,385</span> │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴───────────────────────────┴────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                         │      \u001b[38;5;34m8,192\u001b[0m │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                         │          \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                         │      \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ block (\u001b[38;5;33mBlock\u001b[0m)                   │ ?                         │    \u001b[38;5;34m197,632\u001b[0m │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ block_1 (\u001b[38;5;33mBlock\u001b[0m)                 │ ?                         │    \u001b[38;5;34m197,632\u001b[0m │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ block_2 (\u001b[38;5;33mBlock\u001b[0m)                 │ ?                         │    \u001b[38;5;34m197,632\u001b[0m │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ block_3 (\u001b[38;5;33mBlock\u001b[0m)                 │ ?                         │    \u001b[38;5;34m197,632\u001b[0m │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ layer_normalization_8           │ ?                         │        \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                           │            │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dense_24 (\u001b[38;5;33mDense\u001b[0m)                │ ?                         │      \u001b[38;5;34m8,385\u001b[0m │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                         │          \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴───────────────────────────┴────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">815,553</span> (3.11 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m815,553\u001b[0m (3.11 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">815,553</span> (3.11 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m815,553\u001b[0m (3.11 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = GPT(GPTArgs, causal=True)\n",
    "model = build_model(model, (2, GPTArgs.max_context_length), [0, GPTArgs.vocab_size-1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TArgs:\n",
    "    \"Training Arguments\"\n",
    "    checkpoint:str = \"weights/gpt_shakespeare2/Epoch{epoch}.weights.h5\"\n",
    "    steps_per_epoch:int = 250\n",
    "    eval_steps = 200\n",
    "    \n",
    "    batch_size:int = 64\n",
    "    num_steps:int = 10_000\n",
    "    epochs:int = num_steps//steps_per_epoch\n",
    "\n",
    "    init_lr:float = 8e-4\n",
    "    max_lr:float = 1e-3\n",
    "    min_lr:float = 1e-4\n",
    "    warmup_steps:int = 100\n",
    "    decay_steps:int = 85*num_steps//100\n",
    "\n",
    "    beta1:float = 0.9\n",
    "    beta2:float = 0.99\n",
    "    clipvalue:float = 1e0\n",
    "    weight_decay:float = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': <_PrefetchDataset element_spec=(TensorSpec(shape=(None, None), dtype=tf.int32, name=None), TensorSpec(shape=(None, None), dtype=tf.int32, name=None))>, 'val': <_PrefetchDataset element_spec=(TensorSpec(shape=(None, None), dtype=tf.int32, name=None), TensorSpec(shape=(None, None), dtype=tf.int32, name=None))>}\n"
     ]
    }
   ],
   "source": [
    "data = encode(shakespeare_text)\n",
    "datasets = {\"train\": to_dataset(data[:1_000_000], length=GPTArgs.max_context_length, shuffle=True, batch_size=TArgs.batch_size, seed=42), # ~90%split\n",
    "            \"val\": to_dataset(data[1_000_000:], length=GPTArgs.max_context_length, shuffle=True, batch_size=TArgs.batch_size, seed=0)}\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model, lr schedule, compile and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = nn.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=TArgs.max_lr,\n",
    "    decay_steps=TArgs.decay_steps,\n",
    "    warmup_steps=TArgs.warmup_steps,\n",
    "    warmup_target=TArgs.max_lr,\n",
    "    alpha=TArgs.min_lr/TArgs.max_lr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = jnp.arange(1, TArgs.num_steps+1)\n",
    "lrs = jax.vmap(learning_rate)(steps)\n",
    "\n",
    "plt.plot(steps, lrs); del lrs, steps\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=nn.optimizers.AdamW(\n",
    "        learning_rate=learning_rate,\n",
    "        beta_1=TArgs.beta1,\n",
    "        beta_2=TArgs.beta2, \n",
    "        clipvalue=TArgs.clipvalue, \n",
    "        weight_decay=TArgs.weight_decay\n",
    "    ),\n",
    "    loss=nn.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_top_k(key, logits, k):\n",
    "    topk_logits, topk_idx = jax.lax.top_k(logits, k=k)\n",
    "    idx = jrand.categorical(key, topk_logits)\n",
    "    return topk_idx[0][idx]\n",
    "\n",
    "\n",
    "def generate(idx:Array, max_new_tokens:int, top_k:int|None=None, seed:int=42):    \n",
    "    key = jrand.PRNGKey(seed)\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -GPTArgs.max_context_length:] # (B, T)\n",
    "        logits = model(idx_cond, training=False)[:, -1, :]/0.89 # (B, T, vocab_size)[:, -1, :] => (B, vocab_size)\n",
    "        idx_next = jrand.categorical(key, logits) if top_k is None else sample_top_k(key, logits, k=top_k) # (B, 1)\n",
    "        key, _ = jrand.split(key)\n",
    "        idx = jnp.concatenate((idx, idx_next[None]), axis=-1) # (B, T+1)\n",
    "    return idx.tolist()[0]\n",
    "\n",
    "\n",
    "saveW = nn.callbacks.ModelCheckpoint(\n",
    "    filepath=TArgs.checkpoint,\n",
    "    save_weights_only=True\n",
    ")\n",
    "early_stop = nn.callbacks.EarlyStopping(\n",
    "    patience=7,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "# class DisplayCallback(nn.callbacks.Callback):\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         print(\"\\n\")\n",
    "#         print(decode(generate(idx=jnp.array([[0.]]), max_new_tokens=100, top_k=10, seed=epoch)))\n",
    "#         print(\"\\n\")\n",
    "#         return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 32ms/step - loss: 2.5694 - val_loss: 2.3311\n",
      "Epoch 2/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.7354 - val_loss: 2.3002\n",
      "Epoch 3/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.6211 - val_loss: 2.2819\n",
      "Epoch 4/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.5792 - val_loss: 2.2115\n",
      "Epoch 5/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.5687 - val_loss: 2.1711\n",
      "Epoch 6/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.5484 - val_loss: 2.1704\n",
      "Epoch 7/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.5342 - val_loss: 2.1764\n",
      "Epoch 8/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 1.5196 - val_loss: 2.1637\n",
      "Epoch 9/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.5024 - val_loss: 2.1545\n",
      "Epoch 10/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.4889 - val_loss: 2.1383\n",
      "Epoch 11/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.4855 - val_loss: 2.1404\n",
      "Epoch 12/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.4727 - val_loss: 2.1565\n",
      "Epoch 13/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.4571 - val_loss: 2.1444\n",
      "Epoch 14/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.4573 - val_loss: 2.1227\n",
      "Epoch 15/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.4624 - val_loss: 2.1427\n",
      "Epoch 16/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.4532 - val_loss: 2.1662\n",
      "Epoch 17/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.4611 - val_loss: 2.1436\n",
      "Epoch 18/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.4501 - val_loss: 2.1512\n",
      "Epoch 19/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.4526 - val_loss: 2.1667\n",
      "Epoch 20/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.4420 - val_loss: 2.1740\n",
      "Epoch 21/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.4362 - val_loss: 2.1796\n",
      "Epoch 22/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.4297 - val_loss: 2.1197\n",
      "Epoch 23/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.4420 - val_loss: 2.0956\n",
      "Epoch 24/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.4583 - val_loss: 2.0969\n",
      "Epoch 25/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 1.4544 - val_loss: 2.0846\n",
      "Epoch 26/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.4580 - val_loss: 2.0883\n",
      "Epoch 27/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.4637 - val_loss: 2.0787\n",
      "Epoch 28/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.4594 - val_loss: 2.0881\n",
      "Epoch 29/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.4588 - val_loss: 2.0803\n",
      "Epoch 30/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.4673 - val_loss: 2.0805\n",
      "Epoch 31/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.4644 - val_loss: 2.0834\n",
      "Epoch 32/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.4606 - val_loss: 2.0852\n",
      "Epoch 33/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.4662 - val_loss: 2.0819\n",
      "Epoch 34/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.4681 - val_loss: 2.0900\n",
      "Epoch 35/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.4617 - val_loss: 2.0893\n",
      "Epoch 36/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.4571 - val_loss: 2.0877\n",
      "Epoch 37/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 1.4493 - val_loss: 2.0850\n",
      "Epoch 38/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 1.4452 - val_loss: 2.0892\n",
      "Epoch 39/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.4503 - val_loss: 2.0481\n",
      "Epoch 40/40\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - loss: 1.4761 - val_loss: 2.0388\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x=datasets[\"train\"],\n",
    "    epochs=TArgs.epochs,\n",
    "    steps_per_epoch=TArgs.steps_per_epoch,\n",
    "    validation_data=datasets[\"val\"],\n",
    "    callbacks=[saveW, early_stop], #DisplayCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qonom owingh cload you, whips, anddes a ditoners oe-Fr-widacrilness to appear thy shame; let heart, Warwick from the baiter and many feigned by thy grace and laughed there's a dust thou liest, and what kill my lord?\n",
      "\n",
      "KING RICHARD:\n",
      "When no fight.\n",
      "\n",
      "CLARENCE:\n",
      "Who bleance is this that we have, madam: I have to-night.\n",
      "But\n",
      "Ay, young was yourselfsame we are we dear blows. I'll tell we have shadeth the reason but forces to deposed!\n",
      "I can grandsire,\n",
      "And in your trumpets,\n",
      "That dear.\n",
      "\n",
      "WARWICK:\n",
      "And set our hearts of the stronger.\n",
      "\n",
      "POLIXENES:\n",
      "She't nothing to assist the liphanity, take out of him to bed!\n",
      "And, I am no sour houses?\n",
      "O, stand he is suppose,\n",
      "Were is thee, sir.\n",
      "\n",
      "LEONTES:\n",
      "Hereford, murder for me answer\n",
      "Before King Henry, haught of nature shall arraRosliyed of these law in my soul to myself, a lectation of his majesty\n",
      "Myself a taily in thy Lammas-sight,\n",
      "And I cannot answer made him so untimely friend, as we hewd disinherit to be there?\n",
      "\n",
      "HASTINGS:\n",
      "We shows me, but I'll warrant and stay:\n",
      "You \n"
     ]
    }
   ],
   "source": [
    "print(decode(generate(idx=jnp.array([[29]]), max_new_tokens=1000, seed=42, top_k=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He, bomby. its wiprich ones bear now then at.'\n",
      "\n",
      "RORIRICHASNMTIMsS:\n",
      "You are throne, being to die,\n",
      "Open these\n",
      "Is lay the suppose to tent; and not be his fear this seek them and\n",
      "To help! my soul!\n",
      "See, razing thou art from his sorrow,\n",
      "Shall be thousands uses into the traitor's\n",
      "As thou would nothing blood at is the self.\n",
      "\n",
      "OXFORD:\n",
      "Now, my lord,\n",
      "Whose honour's face;\n",
      "For that thou deny to hriek\n",
      "Of my breast of the princely eyes\n",
      "That we have left came from the park to myself?\n",
      "\n",
      "WARWICK:\n",
      "She's from this arm up the\n",
      "Enfranchisements of me\n",
      "May shot seld with death,\n",
      "I have done, master die.\n",
      "\n",
      "POLIXENES:\n",
      "Why thee seldom proceeders,\n",
      "Which we will assured at the say the dulleshines that this place\n",
      "I' quarrel was with me some for brother ancestor, be viand howlrn;\n",
      "And, as mine.\n",
      "\n",
      "KING HENRY VI:\n",
      "What sleep, and as falling on the taunts he was to make me these honourably threat thus toward the shall we met then bring your me got wantings insulting tempeach my friendly,\n",
      "Bulo, thou could day by complices, as my\n"
     ]
    }
   ],
   "source": [
    "print(decode(generate(idx=jnp.array([[20.]]), max_new_tokens=1000, seed=98857745, top_k=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lon\n",
      "I myfento bean wault\n",
      "With onever clase?s sucar,\n",
      "His sigh treason carries and trumpets!\n",
      "\n",
      "PRINCE EDWARD IV:\n",
      "But Warwick. Well, my lord, liether spite of ill-shaped on the king was thine every thing;\n",
      "And sting,\n",
      "And make your house\n",
      "Be and his native the cure of the abject,\n",
      "And set thy crown.\n",
      "\n",
      "KING HENRY VI:\n",
      "What call King Lewis unto the queen\n",
      "Romeo's hand this sin\n",
      "Hast to creature in tenter'd jot.\n",
      "\n",
      "KING EDWARD IV:\n",
      "I will not pectacle thinks it none inforce we favour'd, and not ceaming common doth swell: even you shall remains,--but lies\n",
      "Not safeget;\n",
      "And that rough for this:\n",
      "And, murder, my lords, to yours, if Warwick by tongue,\n",
      "And do it.\n",
      "\n",
      "LEONTES:\n",
      "A blind they to see thing Tybalt;\n",
      "And what's a dauphin is thy face,\n",
      "Writ in that she's could, do evention may brothers daggers than you like to ours must have been wappear to a hown.\n",
      "\n",
      "CLARENCE:\n",
      "Why, lords, there is no thou there is no stops this world forget young Rosaling show'd you slow, with the sight.\n",
      "\n",
      "POLIXENES:\n",
      "Brother for one?\n",
      "\n",
      "Second Watchman:\n",
      "I am they are spake it, and yet next and will acts, sport the lalong,\n",
      "And a king in renown'd;\n",
      "And us hold we were hath done me merry in the prince, I will father's traitor\n",
      "Be patience,\n",
      "Or else he lady to London the common both objects taken.\n",
      "\n",
      "LADY GREY:\n",
      "Each thee thinkit is it\n",
      "is not night\n",
      "Never parent shall be concludate thine.\n",
      "\n",
      "LEONTES:\n",
      "'O, God, I as my deserves is the house of life\n",
      "In which words, come, in to be seen returns and so that son in a lady from the edge\n",
      "Which sworn,\n",
      "Or any tamping trees they saw truth,\n",
      "Which, seeks of your majesty\n",
      "To rase! O, thou native place.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "I came, and so I\n",
      "Wrath Margaret, and I ordain'd.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Let me merry of their woman!\n",
      "Prepare our brothers can to be repossessor for Burgunarring sweet; unshine my tender than trust: as I not stay at the cell a rraven, do some for her children, there I like not, I prush.\n",
      "But he do a fool!\n",
      "A' ha! dare not poor my exercise are all of much is brother?\n",
      "\n",
      "WARWICK:\n",
      "My brother of ourse\n"
     ]
    }
   ],
   "source": [
    "print(decode(generate(idx=jnp.array([[0.]]), max_new_tokens=2000, seed=12212, top_k=None)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
